{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import seaborn\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading\n",
    "- Load csv into three separate pytorch \"DataLoader\"s, train, validation, and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset_ = pd.read_csv('./oracle_data.csv')\n",
    "\n",
    "# Pick out data only where object pose changed\n",
    "changed_inds = (dataset_['y_0'] != dataset_['y\\'_0']) | (dataset_['y_1'] != dataset_['y\\'_1']) | (dataset_['y_2'] != dataset_['y\\'_2'])\n",
    "dataset_ = dataset_[changed_inds]\n",
    "dataset = dataset_.copy()\n",
    "\n",
    "# Ignore robot successor state\n",
    "dataset = dataset[[c for c in dataset.columns if 'x\\'' not in c]]\n",
    "\n",
    "# robot relative position\n",
    "object_pos = dataset[[c for c in dataset.columns if 'y_' in c]]\n",
    "for i in range(2):\n",
    "    dataset.loc[:, 'x_{}'.format(i)] -= dataset['y_{}'.format(i)].values\n",
    "# relative object change\n",
    "for i in range(3):\n",
    "    dataset.loc[:, 'y\\'_{}'.format(i)] -= dataset['y_{}'.format(i)].values\n",
    "# drop initial object x, y\n",
    "dataset = dataset[[c for c in dataset.columns if c not in ['y_0', 'y_1']]]\n",
    "\n",
    "# Remake angles θ to cos(θ), sin(θ). Redo as more automatically?\n",
    "angles = dataset['x_2'].copy()\n",
    "dataset['x_2'] = angles.apply('cos')\n",
    "dataset['x_3'] = angles.apply('sin')\n",
    "angles = dataset['y_2'].copy()\n",
    "dataset['y_2'] = angles.apply('cos')\n",
    "dataset['y_3'] = angles.apply('sin')\n",
    "angles = dataset['y\\'_2'].copy()\n",
    "dataset['y\\'_2'] = angles.apply('cos')\n",
    "dataset['y\\'_3'] = angles.apply('sin')\n",
    "\n",
    "dataset = dataset[['x_0', 'x_1', 'x_2', 'x_3', 'y_2', 'y_3', 'y\\'_0', 'y\\'_1', 'y\\'_2', 'y\\'_3',\n",
    "                   'u_0', 'u_1', 'u_2', 'u_3', 'u_4']]\n",
    "\n",
    "# Decide where to split in training/validation/test\n",
    "train_cut = int(len(dataset) * 0.9)\n",
    "valid_cut = train_cut + int(len(dataset) * 0.05)\n",
    "dataset_train_ = dataset[:train_cut]\n",
    "dataset_val_ = dataset[train_cut:valid_cut]\n",
    "dataset_test_ = dataset[valid_cut:]\n",
    "\n",
    "# Normalize by training set statistics\n",
    "training_µ = dataset_train_.mean()\n",
    "training_σ = dataset_train_.std()\n",
    "dataset_train = (dataset_train_ - training_µ) / training_σ\n",
    "dataset_val = (dataset_train_ - training_µ) / training_σ\n",
    "dataset_test = (dataset_train_ - training_µ) / training_σ\n",
    "\n",
    "def get_dataloader(dataset, batch_size=32):\n",
    "    X = dataset[[name for name in dataset.columns if not name.startswith('u')]]\n",
    "    Y = dataset[[name for name in dataset.columns if name.startswith('u')]]\n",
    "    return DataLoader(\n",
    "        list(zip(X.as_matrix().astype(np.float32), Y.as_matrix().astype(np.float32))),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "dataloader_train = get_dataloader(dataset_train)\n",
    "dataloader_val = get_dataloader(dataset_val, batch_size=128)\n",
    "dataloader_test = get_dataloader(dataset_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_0</th>\n",
       "      <th>x_1</th>\n",
       "      <th>x_2</th>\n",
       "      <th>y_0</th>\n",
       "      <th>y_1</th>\n",
       "      <th>y_2</th>\n",
       "      <th>x'_0</th>\n",
       "      <th>x'_1</th>\n",
       "      <th>x'_2</th>\n",
       "      <th>y'_0</th>\n",
       "      <th>y'_1</th>\n",
       "      <th>y'_2</th>\n",
       "      <th>u_0</th>\n",
       "      <th>u_1</th>\n",
       "      <th>u_2</th>\n",
       "      <th>u_3</th>\n",
       "      <th>u_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.394126</td>\n",
       "      <td>0.994688</td>\n",
       "      <td>-2.819700</td>\n",
       "      <td>0.135810</td>\n",
       "      <td>-0.311288</td>\n",
       "      <td>-2.936730</td>\n",
       "      <td>0.863803</td>\n",
       "      <td>-1.163890</td>\n",
       "      <td>-1.512680</td>\n",
       "      <td>-0.902363</td>\n",
       "      <td>-3.149270</td>\n",
       "      <td>-211.501000</td>\n",
       "      <td>1.112050</td>\n",
       "      <td>-1.944450</td>\n",
       "      <td>1.161550</td>\n",
       "      <td>0.159846</td>\n",
       "      <td>8.00009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>1.715370</td>\n",
       "      <td>0.338092</td>\n",
       "      <td>2.061680</td>\n",
       "      <td>0.330714</td>\n",
       "      <td>0.216791</td>\n",
       "      <td>-2.829220</td>\n",
       "      <td>-0.330708</td>\n",
       "      <td>0.291698</td>\n",
       "      <td>1.534850</td>\n",
       "      <td>-4.228600</td>\n",
       "      <td>2.208120</td>\n",
       "      <td>-57.792200</td>\n",
       "      <td>-1.828690</td>\n",
       "      <td>-0.040563</td>\n",
       "      <td>-0.485500</td>\n",
       "      <td>0.228786</td>\n",
       "      <td>8.00009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.358663</td>\n",
       "      <td>1.132020</td>\n",
       "      <td>-0.775950</td>\n",
       "      <td>0.618190</td>\n",
       "      <td>-0.616505</td>\n",
       "      <td>1.139030</td>\n",
       "      <td>0.569226</td>\n",
       "      <td>-1.934760</td>\n",
       "      <td>0.222031</td>\n",
       "      <td>1.868990</td>\n",
       "      <td>-1.733000</td>\n",
       "      <td>246.450000</td>\n",
       "      <td>0.128419</td>\n",
       "      <td>-1.897350</td>\n",
       "      <td>0.606969</td>\n",
       "      <td>0.692946</td>\n",
       "      <td>8.00009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>1.696870</td>\n",
       "      <td>0.518726</td>\n",
       "      <td>-2.417190</td>\n",
       "      <td>0.871611</td>\n",
       "      <td>0.800647</td>\n",
       "      <td>-2.947050</td>\n",
       "      <td>0.172123</td>\n",
       "      <td>1.041010</td>\n",
       "      <td>-0.887129</td>\n",
       "      <td>-0.746407</td>\n",
       "      <td>1.667410</td>\n",
       "      <td>-79.018400</td>\n",
       "      <td>-0.983225</td>\n",
       "      <td>0.336806</td>\n",
       "      <td>0.984748</td>\n",
       "      <td>0.849604</td>\n",
       "      <td>8.00009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>-0.276851</td>\n",
       "      <td>-1.883570</td>\n",
       "      <td>-2.336900</td>\n",
       "      <td>-0.853011</td>\n",
       "      <td>1.993890</td>\n",
       "      <td>1.665900</td>\n",
       "      <td>1.121380</td>\n",
       "      <td>-1.621500</td>\n",
       "      <td>-2.728040</td>\n",
       "      <td>-0.853012</td>\n",
       "      <td>1.993890</td>\n",
       "      <td>1.665900</td>\n",
       "      <td>1.112370</td>\n",
       "      <td>0.206261</td>\n",
       "      <td>-0.307619</td>\n",
       "      <td>0.715966</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>-1.294730</td>\n",
       "      <td>-0.082392</td>\n",
       "      <td>2.981140</td>\n",
       "      <td>0.729555</td>\n",
       "      <td>0.573355</td>\n",
       "      <td>-0.444367</td>\n",
       "      <td>0.513942</td>\n",
       "      <td>0.662981</td>\n",
       "      <td>3.806890</td>\n",
       "      <td>0.929303</td>\n",
       "      <td>0.538268</td>\n",
       "      <td>-31.676500</td>\n",
       "      <td>1.659560</td>\n",
       "      <td>0.672096</td>\n",
       "      <td>0.744094</td>\n",
       "      <td>0.280313</td>\n",
       "      <td>8.00009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>-1.075390</td>\n",
       "      <td>1.380310</td>\n",
       "      <td>-2.933310</td>\n",
       "      <td>-0.401199</td>\n",
       "      <td>1.709660</td>\n",
       "      <td>2.580980</td>\n",
       "      <td>-3.431010</td>\n",
       "      <td>2.839900</td>\n",
       "      <td>-3.389830</td>\n",
       "      <td>-0.401198</td>\n",
       "      <td>1.709660</td>\n",
       "      <td>2.580980</td>\n",
       "      <td>-1.591840</td>\n",
       "      <td>0.975319</td>\n",
       "      <td>-0.304972</td>\n",
       "      <td>0.701837</td>\n",
       "      <td>0.01000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>-1.876790</td>\n",
       "      <td>1.078790</td>\n",
       "      <td>-1.890780</td>\n",
       "      <td>0.701342</td>\n",
       "      <td>-0.144400</td>\n",
       "      <td>0.931588</td>\n",
       "      <td>-2.919250</td>\n",
       "      <td>0.165891</td>\n",
       "      <td>-2.300510</td>\n",
       "      <td>0.701343</td>\n",
       "      <td>-0.144400</td>\n",
       "      <td>0.931588</td>\n",
       "      <td>-0.963231</td>\n",
       "      <td>-0.836771</td>\n",
       "      <td>-0.375298</td>\n",
       "      <td>0.610587</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>1.302940</td>\n",
       "      <td>1.364180</td>\n",
       "      <td>-1.370900</td>\n",
       "      <td>1.969860</td>\n",
       "      <td>-0.555616</td>\n",
       "      <td>-2.730690</td>\n",
       "      <td>1.908890</td>\n",
       "      <td>-1.593140</td>\n",
       "      <td>-0.444253</td>\n",
       "      <td>4.633950</td>\n",
       "      <td>-2.345490</td>\n",
       "      <td>206.489000</td>\n",
       "      <td>0.398008</td>\n",
       "      <td>-1.973710</td>\n",
       "      <td>0.593960</td>\n",
       "      <td>0.537147</td>\n",
       "      <td>8.00009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>-1.147020</td>\n",
       "      <td>0.672673</td>\n",
       "      <td>1.696920</td>\n",
       "      <td>-1.396370</td>\n",
       "      <td>0.055177</td>\n",
       "      <td>0.553092</td>\n",
       "      <td>-1.056500</td>\n",
       "      <td>1.841500</td>\n",
       "      <td>1.889500</td>\n",
       "      <td>-1.396370</td>\n",
       "      <td>0.055177</td>\n",
       "      <td>0.553092</td>\n",
       "      <td>0.075362</td>\n",
       "      <td>0.982891</td>\n",
       "      <td>0.160244</td>\n",
       "      <td>0.711545</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>-0.038974</td>\n",
       "      <td>-0.972914</td>\n",
       "      <td>0.363680</td>\n",
       "      <td>0.075469</td>\n",
       "      <td>-1.003930</td>\n",
       "      <td>-2.483480</td>\n",
       "      <td>-0.949788</td>\n",
       "      <td>-4.005440</td>\n",
       "      <td>0.861651</td>\n",
       "      <td>-0.849184</td>\n",
       "      <td>-3.918450</td>\n",
       "      <td>-2.280610</td>\n",
       "      <td>-0.554753</td>\n",
       "      <td>-1.877270</td>\n",
       "      <td>0.304125</td>\n",
       "      <td>0.704482</td>\n",
       "      <td>0.02000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>-0.715610</td>\n",
       "      <td>-0.434350</td>\n",
       "      <td>2.681630</td>\n",
       "      <td>0.966002</td>\n",
       "      <td>0.952517</td>\n",
       "      <td>2.309540</td>\n",
       "      <td>1.286110</td>\n",
       "      <td>1.229230</td>\n",
       "      <td>3.320540</td>\n",
       "      <td>5.251990</td>\n",
       "      <td>2.669940</td>\n",
       "      <td>-197.566000</td>\n",
       "      <td>1.481460</td>\n",
       "      <td>1.218550</td>\n",
       "      <td>0.460765</td>\n",
       "      <td>0.625620</td>\n",
       "      <td>8.00009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>-1.731890</td>\n",
       "      <td>0.415056</td>\n",
       "      <td>-1.623360</td>\n",
       "      <td>-1.017410</td>\n",
       "      <td>-0.098191</td>\n",
       "      <td>2.729100</td>\n",
       "      <td>-0.412991</td>\n",
       "      <td>-0.768544</td>\n",
       "      <td>-1.308430</td>\n",
       "      <td>1.437770</td>\n",
       "      <td>0.246212</td>\n",
       "      <td>193.929000</td>\n",
       "      <td>1.413850</td>\n",
       "      <td>-1.251390</td>\n",
       "      <td>0.336597</td>\n",
       "      <td>0.239771</td>\n",
       "      <td>8.00009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>1.303370</td>\n",
       "      <td>-0.177922</td>\n",
       "      <td>2.784760</td>\n",
       "      <td>0.839873</td>\n",
       "      <td>-0.025983</td>\n",
       "      <td>-2.291870</td>\n",
       "      <td>-0.698624</td>\n",
       "      <td>0.346628</td>\n",
       "      <td>3.469830</td>\n",
       "      <td>-1.752560</td>\n",
       "      <td>0.014462</td>\n",
       "      <td>65.748100</td>\n",
       "      <td>-1.668930</td>\n",
       "      <td>0.429238</td>\n",
       "      <td>0.565921</td>\n",
       "      <td>0.388796</td>\n",
       "      <td>8.00009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>-0.549432</td>\n",
       "      <td>-0.826374</td>\n",
       "      <td>-1.767380</td>\n",
       "      <td>0.723846</td>\n",
       "      <td>-1.946990</td>\n",
       "      <td>-2.193310</td>\n",
       "      <td>0.665764</td>\n",
       "      <td>-2.263780</td>\n",
       "      <td>-2.725620</td>\n",
       "      <td>0.723845</td>\n",
       "      <td>-1.946990</td>\n",
       "      <td>-2.193310</td>\n",
       "      <td>0.907674</td>\n",
       "      <td>-1.081740</td>\n",
       "      <td>-0.715195</td>\n",
       "      <td>0.799242</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>-1.546920</td>\n",
       "      <td>-1.506170</td>\n",
       "      <td>-1.778230</td>\n",
       "      <td>-1.716910</td>\n",
       "      <td>-1.921940</td>\n",
       "      <td>1.932780</td>\n",
       "      <td>-2.592600</td>\n",
       "      <td>-2.739890</td>\n",
       "      <td>-2.051590</td>\n",
       "      <td>-2.504230</td>\n",
       "      <td>-3.775790</td>\n",
       "      <td>118.989000</td>\n",
       "      <td>-1.155330</td>\n",
       "      <td>-1.385390</td>\n",
       "      <td>-0.309545</td>\n",
       "      <td>0.213346</td>\n",
       "      <td>8.00009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>1.540490</td>\n",
       "      <td>1.028710</td>\n",
       "      <td>-2.162100</td>\n",
       "      <td>1.701820</td>\n",
       "      <td>1.001740</td>\n",
       "      <td>-2.891360</td>\n",
       "      <td>2.334250</td>\n",
       "      <td>-2.018090</td>\n",
       "      <td>0.227469</td>\n",
       "      <td>2.272770</td>\n",
       "      <td>-0.773291</td>\n",
       "      <td>8.644250</td>\n",
       "      <td>0.465936</td>\n",
       "      <td>-1.788200</td>\n",
       "      <td>1.398640</td>\n",
       "      <td>0.706384</td>\n",
       "      <td>8.00009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>-0.635193</td>\n",
       "      <td>-0.389188</td>\n",
       "      <td>1.120460</td>\n",
       "      <td>-1.107030</td>\n",
       "      <td>-0.508707</td>\n",
       "      <td>1.196590</td>\n",
       "      <td>-3.927320</td>\n",
       "      <td>0.023239</td>\n",
       "      <td>-0.469946</td>\n",
       "      <td>-1.806010</td>\n",
       "      <td>-0.926189</td>\n",
       "      <td>-69.337000</td>\n",
       "      <td>-1.855930</td>\n",
       "      <td>0.229351</td>\n",
       "      <td>-0.891182</td>\n",
       "      <td>0.871561</td>\n",
       "      <td>8.00009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>1.727590</td>\n",
       "      <td>-0.129856</td>\n",
       "      <td>-2.958000</td>\n",
       "      <td>0.001405</td>\n",
       "      <td>-0.997048</td>\n",
       "      <td>-2.968040</td>\n",
       "      <td>0.828491</td>\n",
       "      <td>-0.220572</td>\n",
       "      <td>-2.317030</td>\n",
       "      <td>0.001405</td>\n",
       "      <td>-0.997048</td>\n",
       "      <td>-2.968040</td>\n",
       "      <td>-1.003540</td>\n",
       "      <td>-0.099999</td>\n",
       "      <td>0.705637</td>\n",
       "      <td>0.404561</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>-0.884441</td>\n",
       "      <td>-0.830137</td>\n",
       "      <td>-1.503910</td>\n",
       "      <td>0.363587</td>\n",
       "      <td>-0.758696</td>\n",
       "      <td>2.232370</td>\n",
       "      <td>1.166870</td>\n",
       "      <td>-1.059500</td>\n",
       "      <td>-3.260150</td>\n",
       "      <td>1.397340</td>\n",
       "      <td>0.150565</td>\n",
       "      <td>243.546000</td>\n",
       "      <td>1.528950</td>\n",
       "      <td>-0.170902</td>\n",
       "      <td>-1.310950</td>\n",
       "      <td>0.406770</td>\n",
       "      <td>8.00009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>0.982475</td>\n",
       "      <td>1.016670</td>\n",
       "      <td>-0.595791</td>\n",
       "      <td>-1.323740</td>\n",
       "      <td>1.525940</td>\n",
       "      <td>1.641750</td>\n",
       "      <td>-1.691400</td>\n",
       "      <td>1.697970</td>\n",
       "      <td>-0.560736</td>\n",
       "      <td>-5.690710</td>\n",
       "      <td>1.739300</td>\n",
       "      <td>21.069900</td>\n",
       "      <td>-1.994970</td>\n",
       "      <td>0.499231</td>\n",
       "      <td>0.024061</td>\n",
       "      <td>0.368398</td>\n",
       "      <td>8.00009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>1.521590</td>\n",
       "      <td>1.042220</td>\n",
       "      <td>0.435960</td>\n",
       "      <td>0.396261</td>\n",
       "      <td>-1.810700</td>\n",
       "      <td>1.936260</td>\n",
       "      <td>2.764210</td>\n",
       "      <td>-0.577744</td>\n",
       "      <td>-1.466120</td>\n",
       "      <td>0.396262</td>\n",
       "      <td>-1.810700</td>\n",
       "      <td>1.936260</td>\n",
       "      <td>0.809099</td>\n",
       "      <td>-1.054800</td>\n",
       "      <td>-1.237220</td>\n",
       "      <td>0.653876</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>-0.710293</td>\n",
       "      <td>-1.882700</td>\n",
       "      <td>2.636590</td>\n",
       "      <td>-0.472074</td>\n",
       "      <td>-1.038880</td>\n",
       "      <td>-2.092440</td>\n",
       "      <td>-0.194759</td>\n",
       "      <td>0.351654</td>\n",
       "      <td>2.875440</td>\n",
       "      <td>1.293030</td>\n",
       "      <td>4.264100</td>\n",
       "      <td>-1.251020</td>\n",
       "      <td>0.397840</td>\n",
       "      <td>1.754060</td>\n",
       "      <td>0.185208</td>\n",
       "      <td>0.420167</td>\n",
       "      <td>3.93000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>0.795932</td>\n",
       "      <td>-0.440374</td>\n",
       "      <td>1.280220</td>\n",
       "      <td>-0.388714</td>\n",
       "      <td>-0.359500</td>\n",
       "      <td>0.886231</td>\n",
       "      <td>-1.681700</td>\n",
       "      <td>-0.231047</td>\n",
       "      <td>2.170610</td>\n",
       "      <td>-6.915480</td>\n",
       "      <td>-0.874503</td>\n",
       "      <td>29.191100</td>\n",
       "      <td>-1.815440</td>\n",
       "      <td>0.150837</td>\n",
       "      <td>0.641423</td>\n",
       "      <td>0.481200</td>\n",
       "      <td>8.00009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>-1.558370</td>\n",
       "      <td>0.046250</td>\n",
       "      <td>2.664600</td>\n",
       "      <td>-0.348052</td>\n",
       "      <td>-0.457457</td>\n",
       "      <td>-2.718220</td>\n",
       "      <td>-1.334010</td>\n",
       "      <td>0.687254</td>\n",
       "      <td>2.404260</td>\n",
       "      <td>-0.348052</td>\n",
       "      <td>-0.457458</td>\n",
       "      <td>-2.718220</td>\n",
       "      <td>0.298232</td>\n",
       "      <td>0.863467</td>\n",
       "      <td>-0.345633</td>\n",
       "      <td>0.321876</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>-0.394041</td>\n",
       "      <td>0.341386</td>\n",
       "      <td>-1.311130</td>\n",
       "      <td>1.878030</td>\n",
       "      <td>-1.951180</td>\n",
       "      <td>1.712130</td>\n",
       "      <td>2.008190</td>\n",
       "      <td>-2.129150</td>\n",
       "      <td>-1.423980</td>\n",
       "      <td>3.618180</td>\n",
       "      <td>-5.309910</td>\n",
       "      <td>-3.521680</td>\n",
       "      <td>1.515190</td>\n",
       "      <td>-1.571980</td>\n",
       "      <td>-0.078124</td>\n",
       "      <td>0.800571</td>\n",
       "      <td>8.00009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>-1.740970</td>\n",
       "      <td>-0.786262</td>\n",
       "      <td>-0.829683</td>\n",
       "      <td>-1.714950</td>\n",
       "      <td>-0.789611</td>\n",
       "      <td>0.754622</td>\n",
       "      <td>-1.036800</td>\n",
       "      <td>-0.541633</td>\n",
       "      <td>-1.344200</td>\n",
       "      <td>-1.026770</td>\n",
       "      <td>-0.675064</td>\n",
       "      <td>0.227451</td>\n",
       "      <td>0.836424</td>\n",
       "      <td>0.288917</td>\n",
       "      <td>-0.611699</td>\n",
       "      <td>0.409189</td>\n",
       "      <td>0.16000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>1.520900</td>\n",
       "      <td>0.765718</td>\n",
       "      <td>2.301580</td>\n",
       "      <td>0.220510</td>\n",
       "      <td>1.183600</td>\n",
       "      <td>3.138430</td>\n",
       "      <td>-1.021570</td>\n",
       "      <td>0.672975</td>\n",
       "      <td>3.008950</td>\n",
       "      <td>0.220511</td>\n",
       "      <td>1.183600</td>\n",
       "      <td>3.138430</td>\n",
       "      <td>-1.908400</td>\n",
       "      <td>-0.068378</td>\n",
       "      <td>0.521270</td>\n",
       "      <td>0.403322</td>\n",
       "      <td>0.01000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>0.827740</td>\n",
       "      <td>-1.724700</td>\n",
       "      <td>-1.030390</td>\n",
       "      <td>0.662691</td>\n",
       "      <td>-1.724170</td>\n",
       "      <td>-1.732410</td>\n",
       "      <td>-0.502692</td>\n",
       "      <td>-2.061710</td>\n",
       "      <td>-0.788541</td>\n",
       "      <td>-0.987532</td>\n",
       "      <td>-2.414390</td>\n",
       "      <td>-1.037370</td>\n",
       "      <td>-1.432000</td>\n",
       "      <td>-0.355074</td>\n",
       "      <td>0.256800</td>\n",
       "      <td>0.234657</td>\n",
       "      <td>1.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>-0.002444</td>\n",
       "      <td>-0.316250</td>\n",
       "      <td>2.144030</td>\n",
       "      <td>-0.319022</td>\n",
       "      <td>-0.139539</td>\n",
       "      <td>-0.960981</td>\n",
       "      <td>-0.540739</td>\n",
       "      <td>0.317742</td>\n",
       "      <td>1.208540</td>\n",
       "      <td>-0.747690</td>\n",
       "      <td>0.442108</td>\n",
       "      <td>9.018360</td>\n",
       "      <td>-0.507510</td>\n",
       "      <td>0.597741</td>\n",
       "      <td>-0.905372</td>\n",
       "      <td>0.416200</td>\n",
       "      <td>8.00009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499386</th>\n",
       "      <td>1.180260</td>\n",
       "      <td>1.202190</td>\n",
       "      <td>2.876370</td>\n",
       "      <td>1.329300</td>\n",
       "      <td>1.199420</td>\n",
       "      <td>1.865810</td>\n",
       "      <td>1.596600</td>\n",
       "      <td>0.676385</td>\n",
       "      <td>3.646460</td>\n",
       "      <td>1.714840</td>\n",
       "      <td>0.824574</td>\n",
       "      <td>3.975270</td>\n",
       "      <td>0.432677</td>\n",
       "      <td>-0.546399</td>\n",
       "      <td>0.797018</td>\n",
       "      <td>0.395306</td>\n",
       "      <td>0.14000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499419</th>\n",
       "      <td>1.581650</td>\n",
       "      <td>-1.114300</td>\n",
       "      <td>0.887019</td>\n",
       "      <td>0.926715</td>\n",
       "      <td>-1.558320</td>\n",
       "      <td>2.687520</td>\n",
       "      <td>-1.598610</td>\n",
       "      <td>-3.621720</td>\n",
       "      <td>-0.754996</td>\n",
       "      <td>-4.147280</td>\n",
       "      <td>-3.755330</td>\n",
       "      <td>-103.578000</td>\n",
       "      <td>-1.919370</td>\n",
       "      <td>-1.494160</td>\n",
       "      <td>-0.977806</td>\n",
       "      <td>0.719613</td>\n",
       "      <td>8.00009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499439</th>\n",
       "      <td>-0.774687</td>\n",
       "      <td>1.343140</td>\n",
       "      <td>-2.661180</td>\n",
       "      <td>-1.234030</td>\n",
       "      <td>-0.581983</td>\n",
       "      <td>0.689795</td>\n",
       "      <td>-0.442495</td>\n",
       "      <td>1.404160</td>\n",
       "      <td>-4.664010</td>\n",
       "      <td>-1.234030</td>\n",
       "      <td>-0.581984</td>\n",
       "      <td>0.689795</td>\n",
       "      <td>0.256857</td>\n",
       "      <td>0.047186</td>\n",
       "      <td>-1.538140</td>\n",
       "      <td>0.203618</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499503</th>\n",
       "      <td>1.090370</td>\n",
       "      <td>-0.117869</td>\n",
       "      <td>-1.130900</td>\n",
       "      <td>0.448309</td>\n",
       "      <td>0.495294</td>\n",
       "      <td>0.650642</td>\n",
       "      <td>-2.174080</td>\n",
       "      <td>2.480820</td>\n",
       "      <td>-1.972850</td>\n",
       "      <td>-3.632110</td>\n",
       "      <td>5.815850</td>\n",
       "      <td>38.498500</td>\n",
       "      <td>-1.770500</td>\n",
       "      <td>1.396000</td>\n",
       "      <td>-0.450552</td>\n",
       "      <td>0.977819</td>\n",
       "      <td>8.00009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499504</th>\n",
       "      <td>1.726680</td>\n",
       "      <td>-0.850285</td>\n",
       "      <td>2.317010</td>\n",
       "      <td>0.881643</td>\n",
       "      <td>-1.013610</td>\n",
       "      <td>2.610040</td>\n",
       "      <td>0.746067</td>\n",
       "      <td>-0.930851</td>\n",
       "      <td>2.469140</td>\n",
       "      <td>0.345426</td>\n",
       "      <td>-1.297990</td>\n",
       "      <td>-172.984000</td>\n",
       "      <td>-1.201950</td>\n",
       "      <td>-0.097007</td>\n",
       "      <td>0.182236</td>\n",
       "      <td>0.231027</td>\n",
       "      <td>8.00009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499528</th>\n",
       "      <td>-1.579700</td>\n",
       "      <td>1.104230</td>\n",
       "      <td>0.552759</td>\n",
       "      <td>-1.659490</td>\n",
       "      <td>1.179440</td>\n",
       "      <td>2.367630</td>\n",
       "      <td>-3.030500</td>\n",
       "      <td>-0.183048</td>\n",
       "      <td>-0.692137</td>\n",
       "      <td>-1.852850</td>\n",
       "      <td>1.228200</td>\n",
       "      <td>-46.177000</td>\n",
       "      <td>-1.145360</td>\n",
       "      <td>-1.015910</td>\n",
       "      <td>-0.982052</td>\n",
       "      <td>0.566661</td>\n",
       "      <td>8.00009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499532</th>\n",
       "      <td>-0.923684</td>\n",
       "      <td>-1.710840</td>\n",
       "      <td>2.729370</td>\n",
       "      <td>-0.733147</td>\n",
       "      <td>-1.915020</td>\n",
       "      <td>-2.626510</td>\n",
       "      <td>2.012440</td>\n",
       "      <td>-1.365190</td>\n",
       "      <td>2.436270</td>\n",
       "      <td>-0.050533</td>\n",
       "      <td>-2.078810</td>\n",
       "      <td>-5.946720</td>\n",
       "      <td>1.957520</td>\n",
       "      <td>0.226360</td>\n",
       "      <td>-0.189031</td>\n",
       "      <td>0.549439</td>\n",
       "      <td>5.95004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499533</th>\n",
       "      <td>1.289930</td>\n",
       "      <td>1.950750</td>\n",
       "      <td>1.539550</td>\n",
       "      <td>0.922858</td>\n",
       "      <td>-0.866677</td>\n",
       "      <td>-2.198190</td>\n",
       "      <td>2.725070</td>\n",
       "      <td>2.988370</td>\n",
       "      <td>2.935090</td>\n",
       "      <td>0.922857</td>\n",
       "      <td>-0.866677</td>\n",
       "      <td>-2.198190</td>\n",
       "      <td>1.085120</td>\n",
       "      <td>0.784558</td>\n",
       "      <td>1.054140</td>\n",
       "      <td>0.571137</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499585</th>\n",
       "      <td>0.876762</td>\n",
       "      <td>0.144851</td>\n",
       "      <td>1.820120</td>\n",
       "      <td>-0.492365</td>\n",
       "      <td>0.129340</td>\n",
       "      <td>0.162681</td>\n",
       "      <td>-0.853688</td>\n",
       "      <td>-2.117540</td>\n",
       "      <td>3.652740</td>\n",
       "      <td>-0.492364</td>\n",
       "      <td>0.129340</td>\n",
       "      <td>0.162681</td>\n",
       "      <td>-1.027640</td>\n",
       "      <td>-1.343540</td>\n",
       "      <td>1.087550</td>\n",
       "      <td>0.908530</td>\n",
       "      <td>0.01000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499595</th>\n",
       "      <td>-0.669136</td>\n",
       "      <td>0.067011</td>\n",
       "      <td>0.932176</td>\n",
       "      <td>-1.043520</td>\n",
       "      <td>-0.007551</td>\n",
       "      <td>-0.751301</td>\n",
       "      <td>-3.622390</td>\n",
       "      <td>-2.038770</td>\n",
       "      <td>2.000290</td>\n",
       "      <td>-3.321190</td>\n",
       "      <td>-0.074177</td>\n",
       "      <td>-139.081000</td>\n",
       "      <td>-1.710660</td>\n",
       "      <td>-1.205740</td>\n",
       "      <td>0.624488</td>\n",
       "      <td>0.892371</td>\n",
       "      <td>8.00009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499612</th>\n",
       "      <td>-1.569840</td>\n",
       "      <td>0.148867</td>\n",
       "      <td>-0.188504</td>\n",
       "      <td>-0.911145</td>\n",
       "      <td>0.484048</td>\n",
       "      <td>1.758090</td>\n",
       "      <td>1.965310</td>\n",
       "      <td>3.046720</td>\n",
       "      <td>0.774459</td>\n",
       "      <td>4.283940</td>\n",
       "      <td>3.054930</td>\n",
       "      <td>3.922260</td>\n",
       "      <td>1.898030</td>\n",
       "      <td>1.539000</td>\n",
       "      <td>0.542864</td>\n",
       "      <td>0.935190</td>\n",
       "      <td>6.37005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499633</th>\n",
       "      <td>-1.346810</td>\n",
       "      <td>-1.608250</td>\n",
       "      <td>-0.887079</td>\n",
       "      <td>-1.614900</td>\n",
       "      <td>-1.135530</td>\n",
       "      <td>-1.631840</td>\n",
       "      <td>-2.697110</td>\n",
       "      <td>0.622480</td>\n",
       "      <td>-0.780232</td>\n",
       "      <td>-3.723410</td>\n",
       "      <td>1.370950</td>\n",
       "      <td>21.580400</td>\n",
       "      <td>-0.996434</td>\n",
       "      <td>1.670340</td>\n",
       "      <td>0.081091</td>\n",
       "      <td>0.521212</td>\n",
       "      <td>8.00009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499638</th>\n",
       "      <td>1.538980</td>\n",
       "      <td>0.257626</td>\n",
       "      <td>0.495713</td>\n",
       "      <td>0.749226</td>\n",
       "      <td>1.883970</td>\n",
       "      <td>0.425510</td>\n",
       "      <td>2.216120</td>\n",
       "      <td>0.521949</td>\n",
       "      <td>0.328652</td>\n",
       "      <td>0.749227</td>\n",
       "      <td>1.883970</td>\n",
       "      <td>0.425510</td>\n",
       "      <td>0.879599</td>\n",
       "      <td>0.338951</td>\n",
       "      <td>-0.214045</td>\n",
       "      <td>0.341317</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499657</th>\n",
       "      <td>0.531689</td>\n",
       "      <td>1.088820</td>\n",
       "      <td>-2.198070</td>\n",
       "      <td>0.630298</td>\n",
       "      <td>0.712427</td>\n",
       "      <td>-1.220920</td>\n",
       "      <td>0.100749</td>\n",
       "      <td>-0.493731</td>\n",
       "      <td>-1.512950</td>\n",
       "      <td>0.640940</td>\n",
       "      <td>-1.104260</td>\n",
       "      <td>8.174710</td>\n",
       "      <td>-0.450770</td>\n",
       "      <td>-1.694720</td>\n",
       "      <td>0.700554</td>\n",
       "      <td>0.109684</td>\n",
       "      <td>8.00009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499722</th>\n",
       "      <td>-1.301490</td>\n",
       "      <td>0.661049</td>\n",
       "      <td>-1.792530</td>\n",
       "      <td>-1.122360</td>\n",
       "      <td>0.568894</td>\n",
       "      <td>-0.889538</td>\n",
       "      <td>-1.085920</td>\n",
       "      <td>1.717180</td>\n",
       "      <td>-1.911290</td>\n",
       "      <td>-1.066880</td>\n",
       "      <td>0.595799</td>\n",
       "      <td>-56.966000</td>\n",
       "      <td>0.250877</td>\n",
       "      <td>1.251660</td>\n",
       "      <td>-0.137603</td>\n",
       "      <td>0.234698</td>\n",
       "      <td>8.00009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499818</th>\n",
       "      <td>1.455960</td>\n",
       "      <td>1.969400</td>\n",
       "      <td>2.937460</td>\n",
       "      <td>1.933270</td>\n",
       "      <td>1.544010</td>\n",
       "      <td>-0.071546</td>\n",
       "      <td>1.801350</td>\n",
       "      <td>1.485690</td>\n",
       "      <td>2.806230</td>\n",
       "      <td>2.200980</td>\n",
       "      <td>1.323390</td>\n",
       "      <td>-48.082800</td>\n",
       "      <td>0.431868</td>\n",
       "      <td>-0.609249</td>\n",
       "      <td>-0.165709</td>\n",
       "      <td>0.496689</td>\n",
       "      <td>8.00009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499822</th>\n",
       "      <td>1.029620</td>\n",
       "      <td>-0.200590</td>\n",
       "      <td>0.637594</td>\n",
       "      <td>-0.056660</td>\n",
       "      <td>0.607334</td>\n",
       "      <td>-2.576410</td>\n",
       "      <td>0.095104</td>\n",
       "      <td>0.743740</td>\n",
       "      <td>1.180250</td>\n",
       "      <td>-0.962985</td>\n",
       "      <td>1.276660</td>\n",
       "      <td>69.233400</td>\n",
       "      <td>-0.929393</td>\n",
       "      <td>0.942105</td>\n",
       "      <td>0.504447</td>\n",
       "      <td>0.540584</td>\n",
       "      <td>8.00009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499844</th>\n",
       "      <td>-1.505250</td>\n",
       "      <td>-0.278770</td>\n",
       "      <td>1.836490</td>\n",
       "      <td>-1.925200</td>\n",
       "      <td>0.283903</td>\n",
       "      <td>0.101371</td>\n",
       "      <td>0.558933</td>\n",
       "      <td>-2.319090</td>\n",
       "      <td>-0.179303</td>\n",
       "      <td>-1.925210</td>\n",
       "      <td>0.283903</td>\n",
       "      <td>0.101371</td>\n",
       "      <td>1.409030</td>\n",
       "      <td>-1.392740</td>\n",
       "      <td>-1.375050</td>\n",
       "      <td>0.484006</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499846</th>\n",
       "      <td>0.398851</td>\n",
       "      <td>-1.195860</td>\n",
       "      <td>-1.763810</td>\n",
       "      <td>0.515907</td>\n",
       "      <td>-1.112210</td>\n",
       "      <td>0.144116</td>\n",
       "      <td>-0.309222</td>\n",
       "      <td>-1.024780</td>\n",
       "      <td>-2.318790</td>\n",
       "      <td>0.439669</td>\n",
       "      <td>-0.998697</td>\n",
       "      <td>-17.388800</td>\n",
       "      <td>-0.739274</td>\n",
       "      <td>0.178951</td>\n",
       "      <td>-0.573718</td>\n",
       "      <td>0.549741</td>\n",
       "      <td>8.00009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499864</th>\n",
       "      <td>-1.108100</td>\n",
       "      <td>-0.723210</td>\n",
       "      <td>-0.953334</td>\n",
       "      <td>-1.025690</td>\n",
       "      <td>-0.409467</td>\n",
       "      <td>0.971226</td>\n",
       "      <td>0.122495</td>\n",
       "      <td>1.787890</td>\n",
       "      <td>1.888990</td>\n",
       "      <td>-0.429588</td>\n",
       "      <td>1.423490</td>\n",
       "      <td>22.851600</td>\n",
       "      <td>0.654412</td>\n",
       "      <td>1.335390</td>\n",
       "      <td>1.509300</td>\n",
       "      <td>0.804412</td>\n",
       "      <td>8.00009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499889</th>\n",
       "      <td>-0.338799</td>\n",
       "      <td>1.456570</td>\n",
       "      <td>0.269783</td>\n",
       "      <td>-1.974560</td>\n",
       "      <td>-1.838910</td>\n",
       "      <td>-1.822990</td>\n",
       "      <td>-1.229960</td>\n",
       "      <td>3.220960</td>\n",
       "      <td>-1.308530</td>\n",
       "      <td>-1.974560</td>\n",
       "      <td>-1.838920</td>\n",
       "      <td>-1.822990</td>\n",
       "      <td>-0.744301</td>\n",
       "      <td>1.473620</td>\n",
       "      <td>-1.317010</td>\n",
       "      <td>0.257864</td>\n",
       "      <td>0.02000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499924</th>\n",
       "      <td>0.025714</td>\n",
       "      <td>-1.369320</td>\n",
       "      <td>-1.362800</td>\n",
       "      <td>0.038835</td>\n",
       "      <td>-1.306340</td>\n",
       "      <td>-2.214240</td>\n",
       "      <td>1.073030</td>\n",
       "      <td>-2.668000</td>\n",
       "      <td>-1.964920</td>\n",
       "      <td>1.104880</td>\n",
       "      <td>-2.538800</td>\n",
       "      <td>-3.536320</td>\n",
       "      <td>0.787238</td>\n",
       "      <td>-0.984491</td>\n",
       "      <td>-0.451530</td>\n",
       "      <td>0.839212</td>\n",
       "      <td>0.04000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499927</th>\n",
       "      <td>1.770980</td>\n",
       "      <td>-0.690841</td>\n",
       "      <td>-2.870430</td>\n",
       "      <td>-0.609178</td>\n",
       "      <td>0.881862</td>\n",
       "      <td>0.461221</td>\n",
       "      <td>-1.396900</td>\n",
       "      <td>1.789900</td>\n",
       "      <td>-3.820810</td>\n",
       "      <td>-2.513850</td>\n",
       "      <td>0.706688</td>\n",
       "      <td>-103.333000</td>\n",
       "      <td>-1.960820</td>\n",
       "      <td>1.517500</td>\n",
       "      <td>-0.584023</td>\n",
       "      <td>0.655378</td>\n",
       "      <td>8.00009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499931</th>\n",
       "      <td>-1.886790</td>\n",
       "      <td>-1.386090</td>\n",
       "      <td>-1.679410</td>\n",
       "      <td>-1.603610</td>\n",
       "      <td>-1.825330</td>\n",
       "      <td>2.822870</td>\n",
       "      <td>-3.085340</td>\n",
       "      <td>0.949711</td>\n",
       "      <td>-1.813620</td>\n",
       "      <td>-1.603620</td>\n",
       "      <td>-1.825330</td>\n",
       "      <td>2.822870</td>\n",
       "      <td>-0.896291</td>\n",
       "      <td>1.772780</td>\n",
       "      <td>-0.100455</td>\n",
       "      <td>0.451962</td>\n",
       "      <td>0.01000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499941</th>\n",
       "      <td>1.030160</td>\n",
       "      <td>-1.735310</td>\n",
       "      <td>0.836459</td>\n",
       "      <td>0.284641</td>\n",
       "      <td>1.758810</td>\n",
       "      <td>-0.286569</td>\n",
       "      <td>2.196950</td>\n",
       "      <td>-0.184798</td>\n",
       "      <td>-1.779670</td>\n",
       "      <td>0.284641</td>\n",
       "      <td>1.758820</td>\n",
       "      <td>-0.286569</td>\n",
       "      <td>0.669232</td>\n",
       "      <td>0.889318</td>\n",
       "      <td>-1.498570</td>\n",
       "      <td>0.675552</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499945</th>\n",
       "      <td>-1.398220</td>\n",
       "      <td>1.609100</td>\n",
       "      <td>-1.725600</td>\n",
       "      <td>-1.317940</td>\n",
       "      <td>1.459830</td>\n",
       "      <td>0.138163</td>\n",
       "      <td>-2.118460</td>\n",
       "      <td>-0.564332</td>\n",
       "      <td>-0.249985</td>\n",
       "      <td>-1.960890</td>\n",
       "      <td>-0.795408</td>\n",
       "      <td>-20.339000</td>\n",
       "      <td>-0.463792</td>\n",
       "      <td>-1.417460</td>\n",
       "      <td>0.946326</td>\n",
       "      <td>0.845995</td>\n",
       "      <td>8.00009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499950</th>\n",
       "      <td>0.352445</td>\n",
       "      <td>-0.429145</td>\n",
       "      <td>2.398020</td>\n",
       "      <td>0.435654</td>\n",
       "      <td>0.482704</td>\n",
       "      <td>-2.650670</td>\n",
       "      <td>0.519315</td>\n",
       "      <td>2.902740</td>\n",
       "      <td>4.679310</td>\n",
       "      <td>1.735690</td>\n",
       "      <td>5.710610</td>\n",
       "      <td>-69.348700</td>\n",
       "      <td>0.088732</td>\n",
       "      <td>1.793400</td>\n",
       "      <td>1.211850</td>\n",
       "      <td>0.985987</td>\n",
       "      <td>8.00009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499956</th>\n",
       "      <td>-0.814094</td>\n",
       "      <td>-0.010050</td>\n",
       "      <td>-0.373610</td>\n",
       "      <td>0.652884</td>\n",
       "      <td>-1.511800</td>\n",
       "      <td>2.080690</td>\n",
       "      <td>1.149000</td>\n",
       "      <td>-2.380030</td>\n",
       "      <td>-1.761170</td>\n",
       "      <td>4.236910</td>\n",
       "      <td>-1.904290</td>\n",
       "      <td>118.494000</td>\n",
       "      <td>1.412150</td>\n",
       "      <td>-1.725030</td>\n",
       "      <td>-1.001830</td>\n",
       "      <td>0.528744</td>\n",
       "      <td>8.00009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499970</th>\n",
       "      <td>0.052601</td>\n",
       "      <td>1.870590</td>\n",
       "      <td>-2.921540</td>\n",
       "      <td>-0.338711</td>\n",
       "      <td>1.467460</td>\n",
       "      <td>0.269144</td>\n",
       "      <td>-1.401250</td>\n",
       "      <td>-0.077675</td>\n",
       "      <td>-1.087180</td>\n",
       "      <td>-2.137160</td>\n",
       "      <td>-0.620179</td>\n",
       "      <td>-159.373000</td>\n",
       "      <td>-1.035850</td>\n",
       "      <td>-1.388130</td>\n",
       "      <td>1.327170</td>\n",
       "      <td>0.456987</td>\n",
       "      <td>8.00009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499995</th>\n",
       "      <td>0.586920</td>\n",
       "      <td>-0.785919</td>\n",
       "      <td>-1.013280</td>\n",
       "      <td>-0.005252</td>\n",
       "      <td>0.051736</td>\n",
       "      <td>-1.706580</td>\n",
       "      <td>-1.676680</td>\n",
       "      <td>1.520220</td>\n",
       "      <td>-1.677850</td>\n",
       "      <td>-4.077640</td>\n",
       "      <td>4.012270</td>\n",
       "      <td>-203.172000</td>\n",
       "      <td>-1.749440</td>\n",
       "      <td>1.800570</td>\n",
       "      <td>-0.483454</td>\n",
       "      <td>0.398151</td>\n",
       "      <td>8.00009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30222 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             x_0       x_1       x_2       y_0       y_1       y_2      x'_0  \\\n",
       "28     -0.394126  0.994688 -2.819700  0.135810 -0.311288 -2.936730  0.863803   \n",
       "81      1.715370  0.338092  2.061680  0.330714  0.216791 -2.829220 -0.330708   \n",
       "96      0.358663  1.132020 -0.775950  0.618190 -0.616505  1.139030  0.569226   \n",
       "109     1.696870  0.518726 -2.417190  0.871611  0.800647 -2.947050  0.172123   \n",
       "139    -0.276851 -1.883570 -2.336900 -0.853011  1.993890  1.665900  1.121380   \n",
       "141    -1.294730 -0.082392  2.981140  0.729555  0.573355 -0.444367  0.513942   \n",
       "167    -1.075390  1.380310 -2.933310 -0.401199  1.709660  2.580980 -3.431010   \n",
       "185    -1.876790  1.078790 -1.890780  0.701342 -0.144400  0.931588 -2.919250   \n",
       "199     1.302940  1.364180 -1.370900  1.969860 -0.555616 -2.730690  1.908890   \n",
       "202    -1.147020  0.672673  1.696920 -1.396370  0.055177  0.553092 -1.056500   \n",
       "209    -0.038974 -0.972914  0.363680  0.075469 -1.003930 -2.483480 -0.949788   \n",
       "222    -0.715610 -0.434350  2.681630  0.966002  0.952517  2.309540  1.286110   \n",
       "224    -1.731890  0.415056 -1.623360 -1.017410 -0.098191  2.729100 -0.412991   \n",
       "235     1.303370 -0.177922  2.784760  0.839873 -0.025983 -2.291870 -0.698624   \n",
       "275    -0.549432 -0.826374 -1.767380  0.723846 -1.946990 -2.193310  0.665764   \n",
       "291    -1.546920 -1.506170 -1.778230 -1.716910 -1.921940  1.932780 -2.592600   \n",
       "309     1.540490  1.028710 -2.162100  1.701820  1.001740 -2.891360  2.334250   \n",
       "313    -0.635193 -0.389188  1.120460 -1.107030 -0.508707  1.196590 -3.927320   \n",
       "332     1.727590 -0.129856 -2.958000  0.001405 -0.997048 -2.968040  0.828491   \n",
       "337    -0.884441 -0.830137 -1.503910  0.363587 -0.758696  2.232370  1.166870   \n",
       "343     0.982475  1.016670 -0.595791 -1.323740  1.525940  1.641750 -1.691400   \n",
       "363     1.521590  1.042220  0.435960  0.396261 -1.810700  1.936260  2.764210   \n",
       "386    -0.710293 -1.882700  2.636590 -0.472074 -1.038880 -2.092440 -0.194759   \n",
       "395     0.795932 -0.440374  1.280220 -0.388714 -0.359500  0.886231 -1.681700   \n",
       "429    -1.558370  0.046250  2.664600 -0.348052 -0.457457 -2.718220 -1.334010   \n",
       "457    -0.394041  0.341386 -1.311130  1.878030 -1.951180  1.712130  2.008190   \n",
       "459    -1.740970 -0.786262 -0.829683 -1.714950 -0.789611  0.754622 -1.036800   \n",
       "472     1.520900  0.765718  2.301580  0.220510  1.183600  3.138430 -1.021570   \n",
       "473     0.827740 -1.724700 -1.030390  0.662691 -1.724170 -1.732410 -0.502692   \n",
       "474    -0.002444 -0.316250  2.144030 -0.319022 -0.139539 -0.960981 -0.540739   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "499386  1.180260  1.202190  2.876370  1.329300  1.199420  1.865810  1.596600   \n",
       "499419  1.581650 -1.114300  0.887019  0.926715 -1.558320  2.687520 -1.598610   \n",
       "499439 -0.774687  1.343140 -2.661180 -1.234030 -0.581983  0.689795 -0.442495   \n",
       "499503  1.090370 -0.117869 -1.130900  0.448309  0.495294  0.650642 -2.174080   \n",
       "499504  1.726680 -0.850285  2.317010  0.881643 -1.013610  2.610040  0.746067   \n",
       "499528 -1.579700  1.104230  0.552759 -1.659490  1.179440  2.367630 -3.030500   \n",
       "499532 -0.923684 -1.710840  2.729370 -0.733147 -1.915020 -2.626510  2.012440   \n",
       "499533  1.289930  1.950750  1.539550  0.922858 -0.866677 -2.198190  2.725070   \n",
       "499585  0.876762  0.144851  1.820120 -0.492365  0.129340  0.162681 -0.853688   \n",
       "499595 -0.669136  0.067011  0.932176 -1.043520 -0.007551 -0.751301 -3.622390   \n",
       "499612 -1.569840  0.148867 -0.188504 -0.911145  0.484048  1.758090  1.965310   \n",
       "499633 -1.346810 -1.608250 -0.887079 -1.614900 -1.135530 -1.631840 -2.697110   \n",
       "499638  1.538980  0.257626  0.495713  0.749226  1.883970  0.425510  2.216120   \n",
       "499657  0.531689  1.088820 -2.198070  0.630298  0.712427 -1.220920  0.100749   \n",
       "499722 -1.301490  0.661049 -1.792530 -1.122360  0.568894 -0.889538 -1.085920   \n",
       "499818  1.455960  1.969400  2.937460  1.933270  1.544010 -0.071546  1.801350   \n",
       "499822  1.029620 -0.200590  0.637594 -0.056660  0.607334 -2.576410  0.095104   \n",
       "499844 -1.505250 -0.278770  1.836490 -1.925200  0.283903  0.101371  0.558933   \n",
       "499846  0.398851 -1.195860 -1.763810  0.515907 -1.112210  0.144116 -0.309222   \n",
       "499864 -1.108100 -0.723210 -0.953334 -1.025690 -0.409467  0.971226  0.122495   \n",
       "499889 -0.338799  1.456570  0.269783 -1.974560 -1.838910 -1.822990 -1.229960   \n",
       "499924  0.025714 -1.369320 -1.362800  0.038835 -1.306340 -2.214240  1.073030   \n",
       "499927  1.770980 -0.690841 -2.870430 -0.609178  0.881862  0.461221 -1.396900   \n",
       "499931 -1.886790 -1.386090 -1.679410 -1.603610 -1.825330  2.822870 -3.085340   \n",
       "499941  1.030160 -1.735310  0.836459  0.284641  1.758810 -0.286569  2.196950   \n",
       "499945 -1.398220  1.609100 -1.725600 -1.317940  1.459830  0.138163 -2.118460   \n",
       "499950  0.352445 -0.429145  2.398020  0.435654  0.482704 -2.650670  0.519315   \n",
       "499956 -0.814094 -0.010050 -0.373610  0.652884 -1.511800  2.080690  1.149000   \n",
       "499970  0.052601  1.870590 -2.921540 -0.338711  1.467460  0.269144 -1.401250   \n",
       "499995  0.586920 -0.785919 -1.013280 -0.005252  0.051736 -1.706580 -1.676680   \n",
       "\n",
       "            x'_1      x'_2      y'_0      y'_1        y'_2       u_0  \\\n",
       "28     -1.163890 -1.512680 -0.902363 -3.149270 -211.501000  1.112050   \n",
       "81      0.291698  1.534850 -4.228600  2.208120  -57.792200 -1.828690   \n",
       "96     -1.934760  0.222031  1.868990 -1.733000  246.450000  0.128419   \n",
       "109     1.041010 -0.887129 -0.746407  1.667410  -79.018400 -0.983225   \n",
       "139    -1.621500 -2.728040 -0.853012  1.993890    1.665900  1.112370   \n",
       "141     0.662981  3.806890  0.929303  0.538268  -31.676500  1.659560   \n",
       "167     2.839900 -3.389830 -0.401198  1.709660    2.580980 -1.591840   \n",
       "185     0.165891 -2.300510  0.701343 -0.144400    0.931588 -0.963231   \n",
       "199    -1.593140 -0.444253  4.633950 -2.345490  206.489000  0.398008   \n",
       "202     1.841500  1.889500 -1.396370  0.055177    0.553092  0.075362   \n",
       "209    -4.005440  0.861651 -0.849184 -3.918450   -2.280610 -0.554753   \n",
       "222     1.229230  3.320540  5.251990  2.669940 -197.566000  1.481460   \n",
       "224    -0.768544 -1.308430  1.437770  0.246212  193.929000  1.413850   \n",
       "235     0.346628  3.469830 -1.752560  0.014462   65.748100 -1.668930   \n",
       "275    -2.263780 -2.725620  0.723845 -1.946990   -2.193310  0.907674   \n",
       "291    -2.739890 -2.051590 -2.504230 -3.775790  118.989000 -1.155330   \n",
       "309    -2.018090  0.227469  2.272770 -0.773291    8.644250  0.465936   \n",
       "313     0.023239 -0.469946 -1.806010 -0.926189  -69.337000 -1.855930   \n",
       "332    -0.220572 -2.317030  0.001405 -0.997048   -2.968040 -1.003540   \n",
       "337    -1.059500 -3.260150  1.397340  0.150565  243.546000  1.528950   \n",
       "343     1.697970 -0.560736 -5.690710  1.739300   21.069900 -1.994970   \n",
       "363    -0.577744 -1.466120  0.396262 -1.810700    1.936260  0.809099   \n",
       "386     0.351654  2.875440  1.293030  4.264100   -1.251020  0.397840   \n",
       "395    -0.231047  2.170610 -6.915480 -0.874503   29.191100 -1.815440   \n",
       "429     0.687254  2.404260 -0.348052 -0.457458   -2.718220  0.298232   \n",
       "457    -2.129150 -1.423980  3.618180 -5.309910   -3.521680  1.515190   \n",
       "459    -0.541633 -1.344200 -1.026770 -0.675064    0.227451  0.836424   \n",
       "472     0.672975  3.008950  0.220511  1.183600    3.138430 -1.908400   \n",
       "473    -2.061710 -0.788541 -0.987532 -2.414390   -1.037370 -1.432000   \n",
       "474     0.317742  1.208540 -0.747690  0.442108    9.018360 -0.507510   \n",
       "...          ...       ...       ...       ...         ...       ...   \n",
       "499386  0.676385  3.646460  1.714840  0.824574    3.975270  0.432677   \n",
       "499419 -3.621720 -0.754996 -4.147280 -3.755330 -103.578000 -1.919370   \n",
       "499439  1.404160 -4.664010 -1.234030 -0.581984    0.689795  0.256857   \n",
       "499503  2.480820 -1.972850 -3.632110  5.815850   38.498500 -1.770500   \n",
       "499504 -0.930851  2.469140  0.345426 -1.297990 -172.984000 -1.201950   \n",
       "499528 -0.183048 -0.692137 -1.852850  1.228200  -46.177000 -1.145360   \n",
       "499532 -1.365190  2.436270 -0.050533 -2.078810   -5.946720  1.957520   \n",
       "499533  2.988370  2.935090  0.922857 -0.866677   -2.198190  1.085120   \n",
       "499585 -2.117540  3.652740 -0.492364  0.129340    0.162681 -1.027640   \n",
       "499595 -2.038770  2.000290 -3.321190 -0.074177 -139.081000 -1.710660   \n",
       "499612  3.046720  0.774459  4.283940  3.054930    3.922260  1.898030   \n",
       "499633  0.622480 -0.780232 -3.723410  1.370950   21.580400 -0.996434   \n",
       "499638  0.521949  0.328652  0.749227  1.883970    0.425510  0.879599   \n",
       "499657 -0.493731 -1.512950  0.640940 -1.104260    8.174710 -0.450770   \n",
       "499722  1.717180 -1.911290 -1.066880  0.595799  -56.966000  0.250877   \n",
       "499818  1.485690  2.806230  2.200980  1.323390  -48.082800  0.431868   \n",
       "499822  0.743740  1.180250 -0.962985  1.276660   69.233400 -0.929393   \n",
       "499844 -2.319090 -0.179303 -1.925210  0.283903    0.101371  1.409030   \n",
       "499846 -1.024780 -2.318790  0.439669 -0.998697  -17.388800 -0.739274   \n",
       "499864  1.787890  1.888990 -0.429588  1.423490   22.851600  0.654412   \n",
       "499889  3.220960 -1.308530 -1.974560 -1.838920   -1.822990 -0.744301   \n",
       "499924 -2.668000 -1.964920  1.104880 -2.538800   -3.536320  0.787238   \n",
       "499927  1.789900 -3.820810 -2.513850  0.706688 -103.333000 -1.960820   \n",
       "499931  0.949711 -1.813620 -1.603620 -1.825330    2.822870 -0.896291   \n",
       "499941 -0.184798 -1.779670  0.284641  1.758820   -0.286569  0.669232   \n",
       "499945 -0.564332 -0.249985 -1.960890 -0.795408  -20.339000 -0.463792   \n",
       "499950  2.902740  4.679310  1.735690  5.710610  -69.348700  0.088732   \n",
       "499956 -2.380030 -1.761170  4.236910 -1.904290  118.494000  1.412150   \n",
       "499970 -0.077675 -1.087180 -2.137160 -0.620179 -159.373000 -1.035850   \n",
       "499995  1.520220 -1.677850 -4.077640  4.012270 -203.172000 -1.749440   \n",
       "\n",
       "             u_1       u_2       u_3      u_4  \n",
       "28     -1.944450  1.161550  0.159846  8.00009  \n",
       "81     -0.040563 -0.485500  0.228786  8.00009  \n",
       "96     -1.897350  0.606969  0.692946  8.00009  \n",
       "109     0.336806  0.984748  0.849604  8.00009  \n",
       "139     0.206261 -0.307619  0.715966  0.00000  \n",
       "141     0.672096  0.744094  0.280313  8.00009  \n",
       "167     0.975319 -0.304972  0.701837  0.01000  \n",
       "185    -0.836771 -0.375298  0.610587  0.00000  \n",
       "199    -1.973710  0.593960  0.537147  8.00009  \n",
       "202     0.982891  0.160244  0.711545  0.00000  \n",
       "209    -1.877270  0.304125  0.704482  0.02000  \n",
       "222     1.218550  0.460765  0.625620  8.00009  \n",
       "224    -1.251390  0.336597  0.239771  8.00009  \n",
       "235     0.429238  0.565921  0.388796  8.00009  \n",
       "275    -1.081740 -0.715195  0.799242  0.00000  \n",
       "291    -1.385390 -0.309545  0.213346  8.00009  \n",
       "309    -1.788200  1.398640  0.706384  8.00009  \n",
       "313     0.229351 -0.891182  0.871561  8.00009  \n",
       "332    -0.099999  0.705637  0.404561  0.00000  \n",
       "337    -0.170902 -1.310950  0.406770  8.00009  \n",
       "343     0.499231  0.024061  0.368398  8.00009  \n",
       "363    -1.054800 -1.237220  0.653876  0.00000  \n",
       "386     1.754060  0.185208  0.420167  3.93000  \n",
       "395     0.150837  0.641423  0.481200  8.00009  \n",
       "429     0.863467 -0.345633  0.321876  0.00000  \n",
       "457    -1.571980 -0.078124  0.800571  8.00009  \n",
       "459     0.288917 -0.611699  0.409189  0.16000  \n",
       "472    -0.068378  0.521270  0.403322  0.01000  \n",
       "473    -0.355074  0.256800  0.234657  1.50000  \n",
       "474     0.597741 -0.905372  0.416200  8.00009  \n",
       "...          ...       ...       ...      ...  \n",
       "499386 -0.546399  0.797018  0.395306  0.14000  \n",
       "499419 -1.494160 -0.977806  0.719613  8.00009  \n",
       "499439  0.047186 -1.538140  0.203618  0.00000  \n",
       "499503  1.396000 -0.450552  0.977819  8.00009  \n",
       "499504 -0.097007  0.182236  0.231027  8.00009  \n",
       "499528 -1.015910 -0.982052  0.566661  8.00009  \n",
       "499532  0.226360 -0.189031  0.549439  5.95004  \n",
       "499533  0.784558  1.054140  0.571137  0.00000  \n",
       "499585 -1.343540  1.087550  0.908530  0.01000  \n",
       "499595 -1.205740  0.624488  0.892371  8.00009  \n",
       "499612  1.539000  0.542864  0.935190  6.37005  \n",
       "499633  1.670340  0.081091  0.521212  8.00009  \n",
       "499638  0.338951 -0.214045  0.341317  0.00000  \n",
       "499657 -1.694720  0.700554  0.109684  8.00009  \n",
       "499722  1.251660 -0.137603  0.234698  8.00009  \n",
       "499818 -0.609249 -0.165709  0.496689  8.00009  \n",
       "499822  0.942105  0.504447  0.540584  8.00009  \n",
       "499844 -1.392740 -1.375050  0.484006  0.00000  \n",
       "499846  0.178951 -0.573718  0.549741  8.00009  \n",
       "499864  1.335390  1.509300  0.804412  8.00009  \n",
       "499889  1.473620 -1.317010  0.257864  0.02000  \n",
       "499924 -0.984491 -0.451530  0.839212  0.04000  \n",
       "499927  1.517500 -0.584023  0.655378  8.00009  \n",
       "499931  1.772780 -0.100455  0.451962  0.01000  \n",
       "499941  0.889318 -1.498570  0.675552  0.00000  \n",
       "499945 -1.417460  0.946326  0.845995  8.00009  \n",
       "499950  1.793400  1.211850  0.985987  8.00009  \n",
       "499956 -1.725030 -1.001830  0.528744  8.00009  \n",
       "499970 -1.388130  1.327170  0.456987  8.00009  \n",
       "499995  1.800570 -0.483454  0.398151  8.00009  \n",
       "\n",
       "[30222 rows x 17 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# redo same step to get input/output dimensions\n",
    "X = dataset[[name for name in dataset.columns if not name.startswith('u')]]\n",
    "Y = dataset[[name for name in dataset.columns if name.startswith('u')]]\n",
    "x_size = len(X.columns)\n",
    "y_size = len(Y.columns)\n",
    "x_size, y_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define models\n",
    "- Separate mean and variance models, for stability\n",
    "- MSE loss for the mean, gaussian log likelihood for the variance\n",
    "- Learn: $p(u|x_r, x_t, x_t')$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def n_fn(k):\n",
    "    return int(k * (1 + k) / 2)\n",
    "\n",
    "\n",
    "def k_fn(n):\n",
    "    return int(-0.5 + (0.25 + 2 * n) ** 0.5)\n",
    "\n",
    "\n",
    "hidden_size = 256\n",
    "n_residual_units = 2\n",
    "n_cholesky_entries = n_fn(y_size)\n",
    "\n",
    "\n",
    "class CovarianceCholesky(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_features):\n",
    "        super(CovarianceCholesky, self).__init__()\n",
    "        self.k = n_features\n",
    "        self.n = int(self.k * (1 + self.k) / 2)\n",
    "        self.row_inds = []\n",
    "        self.col_inds = []\n",
    "        self.diagonal_inds = list(range(self.k))\n",
    "        for i in range(1, self.k):\n",
    "            for j in range(i):\n",
    "                self.row_inds.append(i)\n",
    "                self.col_inds.append(j)\n",
    "\n",
    "    def forward(self, x):\n",
    "        n = x.size(1)\n",
    "        k = int(-0.5 + (0.25 + 2 * n) ** 0.5)\n",
    "        y = Variable(torch.zeros(x.size(0), k, k))\n",
    "        if x.data.is_cuda:\n",
    "            y = y.cuda()\n",
    "        y[:, self.row_inds, self.col_inds] = x[:, :n - k]\n",
    "        y[:, self.diagonal_inds, self.diagonal_inds] = F.softplus(x[:, n - k:])\n",
    "        return y\n",
    "\n",
    "class Residual(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, num_features):\n",
    "        super(Residual, self).__init__()\n",
    "        self.bn1 = torch.nn.BatchNorm1d(num_features)\n",
    "        self.fc1 = torch.nn.Linear(num_features, num_features)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(num_features)\n",
    "        self.fc2 = torch.nn.Linear(num_features, num_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        a = self.fc1(F.relu(self.bn1(x)))\n",
    "        b = self.fc2(F.relu(self.bn2(a)))\n",
    "        return b + x\n",
    "    \n",
    "\n",
    "mean_model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(x_size, hidden_size),\n",
    "    *[Residual(hidden_size) for _ in range(n_residual_units)],\n",
    "    torch.nn.Linear(hidden_size, y_size)\n",
    ").cuda()\n",
    "\n",
    "var_model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(x_size, hidden_size),\n",
    "    *[Residual(hidden_size) for _ in range(n_residual_units)],\n",
    "    torch.nn.Linear(hidden_size, y_size),\n",
    "    torch.nn.Softplus()\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gaussian_likelihood():\n",
    "    def f(µ, σ, y):\n",
    "        return (torch.log(σ) + 0.5 * (µ - y) * (µ - y) / (σ ** 2)).sum()\n",
    "    return f\n",
    "\n",
    "def multivariate_gaussian_likelihood(µ, L, y):\n",
    "    \"\"\"(Average) Negative log-likelihood of a multivariate gaussian.\n",
    "    \n",
    "    Notation below:\n",
    "    n = Number of samples\n",
    "    k = Dimensionality of gaussian variables\n",
    "\n",
    "    µ : Variable, dimensions: (n, k)\n",
    "    L : Lower triangular cholesky decomposition of the covariance matrices, that is LL' = Σ\n",
    "        dimensions: (n, k, k)\n",
    "    y : Variable, dimensions: (n, k)\n",
    "    \"\"\"\n",
    "    n, k = µ.size()\n",
    "    nll = Variable(torch.zeros(1))\n",
    "    if µ.data.is_cuda:\n",
    "        nll = nll.cuda()\n",
    "    for i in range(n):\n",
    "        l = L[i, :, :]\n",
    "        L_inv = torch.inverse(l)\n",
    "        Σ_inv = L_inv.transpose(0, 1) @ L_inv\n",
    "        d = y[i:i + 1, :] - µ[i:i + 1, :]\n",
    "        logdet = 2 * torch.log(l.diag()).sum()\n",
    "        nll += 0.5 * (\n",
    "            logdet +\n",
    "            (d @ Σ_inv @ d.transpose(0, 1)).sum() +\n",
    "            k * np.log(2 * np.pi)\n",
    "        )\n",
    "    return nll / n\n",
    "\n",
    "mean_loss_fn = torch.nn.MSELoss()\n",
    "mean_optim = torch.optim.Adam(mean_model.parameters(), weight_decay=1e-4)\n",
    "var_loss_fn = gaussian_likelihood()\n",
    "var_optim = torch.optim.Adam(var_model.parameters(), weight_decay=1e-4)\n",
    "\n",
    "time_str = datetime.datetime.now().strftime('%H:%M')\n",
    "logger_train = SummaryWriter('runs/resid-2-hidden-256-wd-1e-4-time-{}-relative-train'.format(time_str))\n",
    "logger_valid = SummaryWriter('runs/resid-2-hidden-256-wd-1e-4-time-{}-relative-valid'.format(time_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-dae249c7afd0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mmse_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_loss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_mean_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mnll_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvar_loss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_mean_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_var_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mlogger_valid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-044f85637e5c>\u001b[0m in \u001b[0;36mf\u001b[0;34m(μ, σ, y)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgaussian_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mµ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mσ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mσ\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mµ\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mµ\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mσ\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/pytorch/torch/autograd/variable.py\u001b[0m in \u001b[0;36m__pow__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__pow__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 853\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__ipow__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/pytorch/torch/autograd/variable.py\u001b[0m in \u001b[0;36mpow\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPowConstant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/pytorch/torch/autograd/_functions/basic_ops.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, a, b)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mPowConstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFunction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_first\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msort_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_val = np.inf\n",
    "best_mean_model = None\n",
    "best_var_model = None\n",
    "step = 0\n",
    "for _ in range(1024):\n",
    "    for batch in dataloader_train:\n",
    "        mean_model.train()\n",
    "        var_model.train()\n",
    "        X, Y = map(lambda x: torch.autograd.Variable(x.cuda()), batch)\n",
    "        Y_mean_pred = mean_model(X)\n",
    "        Y_var_pred = var_model(X)\n",
    "\n",
    "        mean_model.zero_grad()\n",
    "        var_model.zero_grad()\n",
    "\n",
    "        mse = mean_loss_fn(Y_mean_pred, Y)\n",
    "        mse.backward(retain_graph=True)\n",
    "        mean_optim.step()\n",
    "\n",
    "        nll = var_loss_fn(Y_mean_pred, Y_var_pred, Y)\n",
    "        nll.backward()\n",
    "        var_optim.step()\n",
    "\n",
    "        if step % 500 == 0:\n",
    "            logger_train.add_scalar('mse', mse.cpu().data.numpy()[0], step)\n",
    "            logger_train.add_scalar('nll', nll.cpu().data.numpy()[0], step)\n",
    "            \n",
    "            mean_model.eval()\n",
    "            var_model.eval()\n",
    "            val_batch = next(iter(dataloader_val))\n",
    "            X, Y = map(lambda x: torch.autograd.Variable(x.cuda()), val_batch)\n",
    "            Y_mean_pred = mean_model(X)\n",
    "            Y_var_pred = var_model(X)\n",
    "\n",
    "            mse_val = mean_loss_fn(Y_mean_pred, Y)\n",
    "            nll_val = var_loss_fn(Y_mean_pred, Y_var_pred, Y).cpu().data.numpy()[0]\n",
    "            \n",
    "            logger_valid.add_scalar('mse', mse_val.cpu().data.numpy()[0], step)\n",
    "            logger_valid.add_scalar('nll', nll_val, step)\n",
    "            \n",
    "            if nll_val < best_val:\n",
    "                best_val = nll_val\n",
    "                best_mean_model = mean_model.state_dict()\n",
    "                best_var_model = var_model.state_dict()\n",
    "\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00.001088\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc911019b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('./python_src/saved_models/oracle_mean_model.pkl', 'rb') as f:\n",
    "    best_mean_model = pickle.load(f)\n",
    "with open('./python_src/saved_models/oracle_var_model.pkl', 'rb') as f:\n",
    "    best_var_model = pickle.load(f)\n",
    "mean_model.load_state_dict(best_mean_model)\n",
    "var_model.load_state_dict(best_var_model)\n",
    "mean_model.eval()\n",
    "var_model.eval()\n",
    "val_batch = next(iter(dataloader_val))\n",
    "X, Y = map(lambda x: torch.autograd.Variable(x.cuda()), val_batch)\n",
    "start_time = datetime.datetime.now()\n",
    "Y_mean_pred = mean_model(X)\n",
    "Y_var_pred = var_model(X)\n",
    "end_time = datetime.datetime.now()\n",
    "print((end_time - start_time))\n",
    "e = (Y_mean_pred - Y).cpu().data.numpy().flatten()\n",
    "ts = np.linspace(-4, 4, 400)\n",
    "handle_normal, = plt.plot(ts, 70 * norm.pdf(ts), alpha=1.0, label='$\\mathcal{N}(0, 1)$', linewidth=1.0)\n",
    "handle_normal2, = plt.plot(ts, 70 * norm.pdf(ts, loc=0, scale=1 / 5.0), alpha=1.0, color='blue', label='$\\mathcal{N}(0, 0.2^2)$', linewidth=1.0)\n",
    "error_handle = mpatches.Patch(color='orange', label='$µ(x) - y$')\n",
    "out = plt.hist(e, bins=np.linspace(-4, 4, 100), color='orange')\n",
    "plt.title('Error distribution on validation set')\n",
    "plt.legend(handles=[handle_normal, handle_normal2, error_handle])\n",
    "#plt.savefig('error_distribution.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#with open('models/oracle_mean_model.pkl', 'wb') as f:\n",
    "#    pickle.dump(best_mean_model, f)\n",
    "#with open('models/oracle_var_model.pkl', 'wb') as f:\n",
    "#    pickle.dump(best_var_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "float_formatter = lambda x: \"%.4f\" % x\n",
    "np.set_printoptions(formatter={'float_kind':float_formatter})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.1121, -1.9445, 1.1616, 0.1598, 8.0001]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#val_batch = next(iter(dataloader_val))\n",
    "#X, Y = map(lambda x: torch.autograd.Variable(x.cuda()), val_batch)\n",
    "X = Variable(\n",
    "    torch.from_numpy(\n",
    "        dataset_train.loc[28][\n",
    "            ['x_0', 'x_1', 'x_2', 'x_3', 'y_2', 'y_3', 'y\\'_0', 'y\\'_1', 'y\\'_2', 'y\\'_3']\n",
    "        ].values.astype(np.float32).reshape(1, -1)\n",
    "    )\n",
    ").cuda()\n",
    "Y = Variable(\n",
    "    torch.from_numpy(\n",
    "        dataset_train.loc[28][\n",
    "            ['u_0', 'u_1', 'u_2', 'u_3', 'u_4']\n",
    "        ].values.astype(np.float32).reshape(1, -1)\n",
    "    )\n",
    ").cuda()\n",
    "Y_mean_pred = mean_model(X).cpu().data.numpy()\n",
    "Y_var_pred = var_model(X).cpu().data.numpy()\n",
    "mean_pred = Y_mean_pred * training_σ.values[-5:] + training_μ.values[-5:]\n",
    "std_pred = Y_var_pred * training_σ.values[-5:]\n",
    "y_true = Y.cpu().data.numpy() * training_σ.values[-5:] + training_µ.values[-5:]\n",
    "mean_pred[0], std_pred\n",
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x_0      -0.394126\n",
       "x_1       0.994688\n",
       "x_2      -2.819700\n",
       "y_0       0.135810\n",
       "y_1      -0.311288\n",
       "y_2      -2.936730\n",
       "x'_0      0.863803\n",
       "x'_1     -1.163890\n",
       "x'_2     -1.512680\n",
       "y'_0     -0.902363\n",
       "y'_1     -3.149270\n",
       "y'_2   -211.501000\n",
       "u_0       1.112050\n",
       "u_1      -1.944450\n",
       "u_2       1.161550\n",
       "u_3       0.159846\n",
       "u_4       8.000090\n",
       "Name: 28, dtype: float64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_.loc[28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#with open('oracle-3-units-256-hidden-mean.pkl', 'wb') as f:\n",
    "#    pickle.dump(mean_model.state_dict(), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pushability\n",
    "$$p(x_t'|x_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset_ = pd.read_csv('./oracle_data.csv')\n",
    "\n",
    "# Pick out data only where object pose changed\n",
    "changed_inds = (dataset_['y_0'] != dataset_['y\\'_0']) | (dataset_['y_1'] != dataset_['y\\'_1']) | (dataset_['y_2'] != dataset_['y\\'_2'])\n",
    "dataset_ = dataset_[changed_inds]\n",
    "dataset = dataset_.copy()\n",
    "\n",
    "# Ignore robot successor state\n",
    "dataset = dataset[[c for c in dataset.columns if 'x\\'' not in c]]\n",
    "\n",
    "# robot relative position\n",
    "object_pos = dataset[[c for c in dataset.columns if 'y_' in c]]\n",
    "for i in range(2):\n",
    "    dataset.loc[:, 'x_{}'.format(i)] -= dataset['y_{}'.format(i)].values\n",
    "# relative object change\n",
    "for i in range(3):\n",
    "    dataset.loc[:, 'y\\'_{}'.format(i)] -= dataset['y_{}'.format(i)].values\n",
    "# drop initial object x, y\n",
    "dataset = dataset[[c for c in dataset.columns if c not in ['y_0', 'y_1']]]\n",
    "\n",
    "# Remake angles θ to cos(θ), sin(θ). Redo as more automatically?\n",
    "angles = dataset['x_2'].copy()\n",
    "dataset['x_2'] = angles.apply('cos')\n",
    "dataset['x_3'] = angles.apply('sin')\n",
    "angles = dataset['y_2'].copy()\n",
    "dataset['y_2'] = angles.apply('cos')\n",
    "dataset['y_3'] = angles.apply('sin')\n",
    "angles = dataset['y\\'_2'].copy()\n",
    "dataset['y\\'_2'] = angles.apply('cos')\n",
    "dataset['y\\'_3'] = angles.apply('sin')\n",
    "\n",
    "dataset = dataset[['y_2', 'y_3', 'y\\'_0', 'y\\'_1', 'y\\'_2', 'y\\'_3']]\n",
    "\n",
    "# Decide where to split in training/validation/test\n",
    "train_cut = int(len(dataset) * 0.9)\n",
    "valid_cut = train_cut + int(len(dataset) * 0.05)\n",
    "dataset_train_ = dataset[:train_cut]\n",
    "dataset_val_ = dataset[train_cut:valid_cut]\n",
    "dataset_test_ = dataset[valid_cut:]\n",
    "\n",
    "# Normalize by training set statistics\n",
    "training_µ = dataset_train_.mean()\n",
    "training_σ = dataset_train_.std()\n",
    "dataset_train = (dataset_train_ - training_µ) / training_σ\n",
    "dataset_val = (dataset_train_ - training_µ) / training_σ\n",
    "dataset_test = (dataset_train_ - training_µ) / training_σ\n",
    "\n",
    "def get_dataloader(dataset, batch_size=32):\n",
    "    X = dataset[[name for name in dataset.columns if name.startswith('y_')]]\n",
    "    Y = dataset[[name for name in dataset.columns if name.startswith('y\\'')]]\n",
    "    return DataLoader(\n",
    "        list(zip(X.as_matrix().astype(np.float32), Y.as_matrix().astype(np.float32))),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "dataloader_train = get_dataloader(dataset_train)\n",
    "dataloader_val = get_dataloader(dataset_val, batch_size=128)\n",
    "dataloader_test = get_dataloader(dataset_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y_2     0.708319\n",
       "y_3     0.705899\n",
       "y'_0    2.017727\n",
       "y'_1    2.025916\n",
       "y'_2    0.737853\n",
       "y'_3    0.580616\n",
       "dtype: float64"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[['y\\'_0', 'y\\'_1', 'y\\'_2', 'y\\'_3']].cov()\n",
    "training_σ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X and Y sizes: 2 4\n"
     ]
    }
   ],
   "source": [
    "X, Y = next(iter(dataloader_train))\n",
    "x_size = X.size(1)\n",
    "y_size = Y.size(1)\n",
    "print('X and Y sizes:', x_size, y_size)\n",
    "pushability_mean = torch.nn.Sequential(\n",
    "    torch.nn.Linear(x_size, hidden_size),\n",
    "    *[Residual(hidden_size) for _ in range(n_residual_units)],\n",
    "    torch.nn.Linear(hidden_size, y_size)\n",
    ").cuda()\n",
    "\n",
    "pushability_var = torch.nn.Sequential(\n",
    "    torch.nn.Linear(x_size, hidden_size),\n",
    "    *[Residual(hidden_size) for _ in range(n_residual_units)],\n",
    "    torch.nn.Linear(hidden_size, y_size),\n",
    "    torch.nn.Softplus()\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_loss_fn = torch.nn.MSELoss()\n",
    "mean_optim = torch.optim.Adam(pushability_mean.parameters(), weight_decay=1e-4)\n",
    "var_loss_fn = gaussian_likelihood()\n",
    "var_optim = torch.optim.Adam(pushability_var.parameters(), weight_decay=1e-4)\n",
    "\n",
    "time_str = datetime.datetime.now().strftime('%H:%M')\n",
    "logger_train = SummaryWriter('runs/resid-3-hidden-256-wd-1e-4-time-{}-pushability-train'.format(time_str))\n",
    "logger_valid = SummaryWriter('runs/resid-3-hidden-256-wd-1e-4-time-{}-pushability-valid'.format(time_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_val = np.inf\n",
    "best_mean_model = None\n",
    "best_var_model = None\n",
    "step = 0\n",
    "for _ in range(1024):\n",
    "    for batch in dataloader_train:\n",
    "        mean_model.train()\n",
    "        pushability_var.train()\n",
    "        X, Y = map(lambda x: torch.autograd.Variable(x.cuda()), batch)\n",
    "        Y_mean_pred = pushability_mean(X)\n",
    "        Y_var_pred = pushability_var(X)\n",
    "\n",
    "        pushability_mean.zero_grad()\n",
    "        pushability_var.zero_grad()\n",
    "\n",
    "        mse = mean_loss_fn(Y_mean_pred, Y)\n",
    "        mse.backward(retain_graph=True)\n",
    "        mean_optim.step()\n",
    "\n",
    "        nll = var_loss_fn(Y_mean_pred, Y_var_pred, Y)\n",
    "        nll.backward()\n",
    "        var_optim.step()\n",
    "\n",
    "        if step % 500 == 0:\n",
    "            logger_train.add_scalar('mse', mse.cpu().data.numpy()[0], step)\n",
    "            logger_train.add_scalar('nll', nll.cpu().data.numpy()[0], step)\n",
    "            \n",
    "            pushability_mean.eval()\n",
    "            pushability_var.eval()\n",
    "            val_batch = next(iter(dataloader_val))\n",
    "            X, Y = map(lambda x: torch.autograd.Variable(x.cuda()), val_batch)\n",
    "            Y_mean_pred = pushability_mean(X)\n",
    "            Y_var_pred = pushability_var(X)\n",
    "\n",
    "            mse_val = mean_loss_fn(Y_mean_pred, Y)\n",
    "            nll_val = var_loss_fn(Y_mean_pred, Y_var_pred, Y).cpu().data.numpy()[0]\n",
    "            \n",
    "            logger_valid.add_scalar('mse', mse_val.cpu().data.numpy()[0], step)\n",
    "            logger_valid.add_scalar('nll', nll_val, step)\n",
    "            \n",
    "            if nll_val < best_val:\n",
    "                best_val = nll_val\n",
    "                best_mean_model = pushability_mean.state_dict()\n",
    "                best_var_model = pushability_var.state_dict()\n",
    "\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       " 1.00000e-02 *\n",
       "   0.8843\n",
       "   0.8165\n",
       "   2.6185\n",
       "  -3.3960\n",
       " [torch.cuda.FloatTensor of size 4 (GPU 0)], Variable containing:\n",
       "  0.9987\n",
       "  1.0139\n",
       "  0.9925\n",
       "  0.9847\n",
       " [torch.cuda.FloatTensor of size 4 (GPU 0)])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_batch = next(iter(dataloader_val))\n",
    "X, Y = map(lambda x: torch.autograd.Variable(x.cuda()), val_batch)\n",
    "Y_mean_pred = pushability_mean(X)\n",
    "Y_var_pred = pushability_var(X)\n",
    "Y_mean_pred[0, :], Y_var_pred[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Super boring model, always predicts $\\mathcal{N}(\\mathbf{0, 1})$, which of course is absolutely correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feasibility\n",
    "Predicts $p(x_{robot}|x_{target}, x_{target}')$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_ = pd.read_csv('./oracle_data.csv')\n",
    "\n",
    "# Pick out data only where object pose changed\n",
    "changed_inds = (dataset_['y_0'] != dataset_['y\\'_0']) | (dataset_['y_1'] != dataset_['y\\'_1']) | (dataset_['y_2'] != dataset_['y\\'_2'])\n",
    "dataset_ = dataset_[changed_inds]\n",
    "dataset = dataset_.copy()\n",
    "\n",
    "# Ignore robot successor state\n",
    "dataset = dataset[[c for c in dataset.columns if 'x\\'' not in c]]\n",
    "\n",
    "# robot relative position\n",
    "object_pos = dataset[[c for c in dataset.columns if 'y_' in c]]\n",
    "for i in range(2):\n",
    "    dataset.loc[:, 'x_{}'.format(i)] -= dataset['y_{}'.format(i)].values\n",
    "# relative object change\n",
    "for i in range(3):\n",
    "    dataset.loc[:, 'y\\'_{}'.format(i)] -= dataset['y_{}'.format(i)].values\n",
    "# drop initial object x, y\n",
    "dataset = dataset[[c for c in dataset.columns if c not in ['y_0', 'y_1']]]\n",
    "\n",
    "# Remake angles θ to cos(θ), sin(θ). Redo as more automatically?\n",
    "angles = dataset['x_2'].copy()\n",
    "dataset['x_2'] = angles.apply('cos')\n",
    "dataset['x_3'] = angles.apply('sin')\n",
    "angles = dataset['y_2'].copy()\n",
    "dataset['y_2'] = angles.apply('cos')\n",
    "dataset['y_3'] = angles.apply('sin')\n",
    "angles = dataset['y\\'_2'].copy()\n",
    "dataset['y\\'_2'] = angles.apply('cos')\n",
    "dataset['y\\'_3'] = angles.apply('sin')\n",
    "\n",
    "dataset = dataset[['x_0', 'x_1', 'x_2', 'x_3', 'y_2', 'y_3', 'y\\'_0', 'y\\'_1', 'y\\'_2', 'y\\'_3']]\n",
    "\n",
    "# Decide where to split in training/validation/test\n",
    "train_cut = int(len(dataset) * 0.9)\n",
    "valid_cut = train_cut + int(len(dataset) * 0.05)\n",
    "dataset_train_ = dataset[:train_cut]\n",
    "dataset_val_ = dataset[train_cut:valid_cut]\n",
    "dataset_test_ = dataset[valid_cut:]\n",
    "\n",
    "# Normalize by training set statistics\n",
    "training_µ = dataset_train_.mean()\n",
    "training_σ = dataset_train_.std()\n",
    "dataset_train = (dataset_train_ - training_µ) / training_σ\n",
    "dataset_val = (dataset_train_ - training_µ) / training_σ\n",
    "dataset_test = (dataset_train_ - training_µ) / training_σ\n",
    "\n",
    "def get_dataloader(dataset, batch_size=32):\n",
    "    X = dataset[[name for name in dataset.columns if name.startswith('y')]]\n",
    "    Y = dataset[[name for name in dataset.columns if name.startswith('x')]]\n",
    "    return DataLoader(\n",
    "        list(zip(X.as_matrix().astype(np.float32), Y.as_matrix().astype(np.float32))),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "dataloader_train = get_dataloader(dataset_train)\n",
    "dataloader_val = get_dataloader(dataset_val, batch_size=128)\n",
    "dataloader_test = get_dataloader(dataset_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X and Y sizes: 6 4\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "n_residual_units = 1\n",
    "\n",
    "X, Y = next(iter(dataloader_train))\n",
    "x_size = X.size(1)\n",
    "y_size = Y.size(1)\n",
    "print('X and Y sizes:', x_size, y_size)\n",
    "feasability_mean = torch.nn.Sequential(\n",
    "    torch.nn.Linear(x_size, hidden_size),\n",
    "    torch.nn.BatchNorm1d(hidden_size),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hidden_size, y_size),\n",
    ").cuda()\n",
    "\n",
    "feasability_var = torch.nn.Sequential(\n",
    "    torch.nn.Linear(x_size, hidden_size),\n",
    "    torch.nn.BatchNorm1d(hidden_size),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hidden_size, y_size),\n",
    "    torch.nn.Softplus()\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_decay = 1e-4\n",
    "mean_loss_fn = torch.nn.MSELoss()\n",
    "mean_optim = torch.optim.Adam(feasability_mean.parameters(), weight_decay=weight_decay)\n",
    "var_loss_fn = gaussian_likelihood()\n",
    "var_optim = torch.optim.Adam(feasability_var.parameters(), weight_decay=weight_decay)\n",
    "\n",
    "time_str = datetime.datetime.now().strftime('%H:%M')\n",
    "name = 'runs/fc-1-hidden-{}-wd-1e-4-time-{}-feasability'.format(\n",
    "    hidden_size, time_str\n",
    ")\n",
    "logger_train = SummaryWriter(name + '-train'.format(n_residual_units, time_str))\n",
    "logger_valid = SummaryWriter(name + '-valid'.format(n_residual_units, time_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-ea868df0926b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mnll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvar_loss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_mean_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_var_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mnll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mvar_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/pytorch/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \"\"\"\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/pytorch/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_val = np.inf\n",
    "best_mean_model = None\n",
    "best_var_model = None\n",
    "step = 0\n",
    "for _ in range(1024):\n",
    "    for batch in dataloader_train:\n",
    "        mean_model.train()\n",
    "        feasability_var.train()\n",
    "        X, Y = map(lambda x: torch.autograd.Variable(x.cuda()), batch)\n",
    "        Y_mean_pred = feasability_mean(X)\n",
    "        Y_var_pred = feasability_var(X)\n",
    "\n",
    "        feasability_mean.zero_grad()\n",
    "        feasability_var.zero_grad()\n",
    "\n",
    "        mse = mean_loss_fn(Y_mean_pred, Y)\n",
    "        mse.backward(retain_graph=True)\n",
    "        mean_optim.step()\n",
    "\n",
    "        nll = var_loss_fn(Y_mean_pred, Y_var_pred, Y)\n",
    "        nll.backward()\n",
    "        var_optim.step()\n",
    "\n",
    "        if step % 500 == 0:\n",
    "            logger_train.add_scalar('mse', mse.cpu().data.numpy()[0], step)\n",
    "            logger_train.add_scalar('nll', nll.cpu().data.numpy()[0], step)\n",
    "            \n",
    "            feasability_mean.eval()\n",
    "            feasability_var.eval()\n",
    "            val_batch = next(iter(dataloader_val))\n",
    "            X, Y = map(lambda x: torch.autograd.Variable(x.cuda()), val_batch)\n",
    "            Y_mean_pred = feasability_mean(X)\n",
    "            Y_var_pred = feasability_var(X)\n",
    "\n",
    "            mse_val = mean_loss_fn(Y_mean_pred, Y)\n",
    "            nll_val = var_loss_fn(Y_mean_pred, Y_var_pred, Y).cpu().data.numpy()[0]\n",
    "            \n",
    "            logger_valid.add_scalar('mse', mse_val.cpu().data.numpy()[0], step)\n",
    "            logger_valid.add_scalar('nll', nll_val, step)\n",
    "            \n",
    "            if nll_val < best_val:\n",
    "                best_val = nll_val\n",
    "                best_mean_model = feasability_mean.state_dict()\n",
    "                best_var_model = feasability_var.state_dict()\n",
    "\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAFpCAYAAACI3gMrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCOb/AYXQUTVMew8PAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc8fa145cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "feasability_mean.load_state_dict(best_mean_model)\n",
    "feasability_var.load_state_dict(best_var_model)\n",
    "\n",
    "#with open('./python_src/saved_models/feasability_mean_model.pkl', 'wb') as f:\n",
    "#    pickle.dump(best_mean_model, f)\n",
    "#with open('./python_src/saved_models/feasability_var_model.pkl', 'wb') as f:\n",
    "#    pickle.dump(best_var_model, f)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "val_batch = next(iter(dataloader_val))\n",
    "X, Y = map(lambda x: torch.autograd.Variable(x.cuda()), val_batch)\n",
    "Y_mean_pred = feasability_mean(X).cpu().data.numpy()\n",
    "Y_var_pred = feasability_var(X).cpu().data.numpy()\n",
    "X = X.cpu().data.numpy()\n",
    "Y = Y.cpu().data.numpy()\n",
    "Y_mean_pred[0, :], Y[0, :], Y_var_pred[0, :]\n",
    "std1 = Ellipse(Y_mean_pred[0, :2], width=Y_var_pred[0, 0] ** 0.5, height=Y_var_pred[0, 1] ** 0.5, alpha=0.5)\n",
    "ax.add_artist(std1)\n",
    "std2 = Ellipse(Y_mean_pred[0, :2], width=2 * Y_var_pred[0, 0] ** 0.5, height=2 * Y_var_pred[0, 1] ** 0.5, alpha=0.5)\n",
    "ax.add_artist(std2)\n",
    "plt.plot(0.0, 0.0, 'go')\n",
    "plt.plot(X[0, 2], X[0, 3], 'ro')\n",
    "plt.plot(Y[0, 0], Y[0, 1], '*')\n",
    "plt.xlim(-5, 5)\n",
    "plt.ylim(-5, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x_0     0.006034\n",
       "x_1    -0.008209\n",
       "x_2    -0.001866\n",
       "x_3     0.001664\n",
       "y_2     0.003809\n",
       "y_3     0.003643\n",
       "y'_0   -0.002864\n",
       "y'_1    0.017553\n",
       "y'_2    0.344222\n",
       "y'_3   -0.001551\n",
       "dtype: float64"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_µ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
